# -*- coding: utf-8 -*-
"""LogisticRegression_SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YEY3ND8A17Q8OkIldUKDNqy3_w85GGlJ
"""



"""#IMDB reviews sentiment analysis project

In this project my aim is to build a machine learning model able to classify movie reviews into *positive* or *negative*, extracting information from the text of the review. This task is known as **Sentiment Analysis**, where you need to identify the sentiment of a text.

Data has been downloaded from [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews), however, the original data is available [here](http://ai.stanford.edu/~amaas/data/sentiment/). 

i Have split data into 3 groups:
- training data: 30,000 reviews annotated with *positive* and *negative* labels;
- validation data: 10,000 reviews annotated with *positive* and *negative* labels;
- test data: 10,000 reviews un-labelled for which you should predict labels for (you will submit your predictions for this test data to the competition server). 

The program should guess whether each review should be labelled as positive or negative. 

For humans it is quite easy to do. For example one review starts: "i can't believe how dumb this movie truly is." which we would rightly assign as being a negative review, but can we program a computer to guess?

## Modules, Functions and Downloads

The following few cells import useful modules, download the datasets and provide a series of useful functions.
"""

# Commented out IPython magic to ensure Python compatibility.
##These are some python libraries that will be useful for your project
#(feel free to just run this cell & move on)
import numpy as np
import urllib.request
from urllib.request import urlopen
import re
from zipfile import ZipFile
from google.colab import files
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd

#classifiers from sklearn. More options here: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning
from sklearn.linear_model import LogisticRegression #Logistic regression classifier: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
from sklearn.neighbors import KNeighborsClassifier #K-neareast neighbors classifier: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification
from sklearn.dummy import DummyClassifier #Dummy classifier (majority class): https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html
from sklearn.naive_bayes import GaussianNB #Naive Bayes classifier: https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes
from sklearn.tree import DecisionTreeClassifier #Decision tree classifier: https://scikit-learn.org/stable/modules/tree.html#classification
from sklearn.ensemble import RandomForestClassifier #Random Forests classifier: https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees
from sklearn.neural_network import MLPClassifier #Multi-layer perceptron classifier: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification

#pre-processing and feature extraction
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

#evaluation
from sklearn.metrics import accuracy_score, confusion_matrix

"""This cell loads the data in the correct splits for the project."""

### DO NOT change this part ###

#loading datasets
df_test = pd.read_csv('https://github.com/carolscarton/headstart/raw/master/data/hs2020_imdb_test.csv')
df_val = pd.read_csv('https://github.com/carolscarton/headstart/raw/master/data/hs2020_imdb_val.csv')
df_train = pd.read_csv('https://github.com/carolscarton/headstart/raw/master/data/hs2020_imdb_train.csv')

"""Pre-processing function."""

########### PRE-PROCESSING ###########
#Pre-processing the reviews
#normalise words, remove punctuation, remove extra spaces, etc
def pre_proc(features):
    processed_features = []
    for sentence in range(0, len(features)):
        # Remove all tags (like <br />)
        processed_feature = re.sub(r'<.*?>', ' ', str(features[sentence]))

        #Remove all special characters
        processed_feature = re.sub(r'[^a-zA-Z0-9]', ' ', processed_feature)

        # Substituting multiple spaces with single space
        processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

        # Removing prefixed 'b'
        processed_feature = re.sub(r'^b\s+', '', processed_feature)
        
        # Removing everything that has numbers 
        processed_feature = re.sub(r'\w*\d\w*', '', processed_feature)

        # Converting to Lowercase
        processed_feature = processed_feature.lower()

        processed_features.append(processed_feature)
    return processed_features

"""Feature extraction function."""

########### EXTRACT FEATURES ###########
# I will use bag-of-words as features for training our classifiers. 
# In a bag-of-words approach, an algorithm counts the number of times a word appear in a document. 
# Each word in the entire collection of documents (corpus) became a feature in the feature vector, which results in a sparse vector.
# Instead of count the "number of times" a word appear in a document, we can also use a binary approach (whether or not a word a appear in a document). 
# if a word appears in a document it will receive 1 (0 otherwise)
def extract_features(proc_train, proc_val, proc_test, binary=False, max_df=1.0, min_df=0.0, ngram_range=(1,1), sw=False):
    stop_words=[]
    if sw:
        f = urlopen("https://raw.githubusercontent.com/lionfish0/discover_stem/master/stopwords.txt").read()
        stop_words = list(np.array(f.split(), dtype=str))

    cv = CountVectorizer(binary=True, max_df=max_df, min_df=min_df, ngram_range=ngram_range, stop_words=stop_words)
    cv.fit(proc_train)

    #check the features outputted below
    #each possible word in our pre-processed vector became a feature
    print("** Vocabulary size: %d" % len(cv.get_feature_names()))
    print("** Words:")
    #print(cv.get_feature_names()) #causes error with colab bandwidth limits

    #apply the model to all data splits
    X_train = cv.transform(proc_train)
    X_val = cv.transform(proc_val)
    X_test = cv.transform(proc_test)
    return X_train, X_val, X_test

"""Evaluation function.

1.   List item
2.   List item
"""

########### EVALUATION ###########
#a function to print a confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() * 0.8
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                      color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax
class_names = ['0', '1', '2', '3', '4']
np.set_printoptions(precision=2)

#evaluation function
def evaluate_classifier(y_test, y_pred):
    print("*** Accuracy score: %.4f\n" % accuracy_score(y_test, y_pred))
    ax = plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')
    plt.plot()

"""Code to create a .zip file for the predictions (answers) of the classifier."""

########### CREATE ZIP FILE FOR CODALAB ###########
#builds and lets you download the answer.zip file
def create_zip (answers):
  from zipfile import ZipFile
  answertext = '\n'.join(answers)
  with open('answer.txt', 'w') as filehandle:
      filehandle.write(answertext)
  with ZipFile('answer.zip', 'w') as zipf:
      zipf.write('answer.txt')    

### only works in Chrome (apparently)
  from google.colab import files
  files.download('answer.zip')

"""<hr>

# The Data

To give an example of the dataset, i have printed the validation set with the labels (i.e. if positive or negative). Below, I have also printed the test set that does not contain labels.
"""

print(df_val)

print(df_test)

"""# Naive Classifier 

Below is an example of a naive solution involving no ML.
"""

naive_classifier = []
for t in df_val['review']:
  if np.any([substr in t.lower() for substr in ['best','wonderful','good','favourite','great','fun','family']]):
    naive_classifier.append('positive')
  else:
    naive_classifier.append('negative')

#evaluate how good the approach is (using the validation set)
evaluate_classifier(df_val['sentiment'], naive_classifier)


naive_classifier_on_test = []
for t in df_test['review']:
  if np.any([substr in t.lower() for substr in ['best','wonderful','good','favourite','great']]):
    naive_classifier_on_test.append('positive')
  else:
    naive_classifier_on_test.append('negative')
#create_zip(naive_classifier_on_test)

"""# K Nearest Neighbours (KNN) Classifier 

Below is an example of a KNN classifier, however this is not a veery good approach for this type of task and yields an accuracy of only 61%.
"""

# apply pre_proc() function to all data splits
proc_train = pre_proc(df_train['review'])
proc_val = pre_proc(df_val['review'])
proc_test = pre_proc(df_test['review'])

# call extract_features() to extract features
# you can play with the parameters of this function do generate different feature vectors:
### binary = True --> means that it will create a binary vector (either a word appears in a review or not)
###          changing it to False will use frequencies (how many times a word appears in a review)
###          it will also normalise it. 
### max_df = ignore terms that have a frequency higher than the value assigned (1.0 means no term is discarded)
### min_df = ignore terms that have a frequency lower than the value assigned (0.0 means no term is discarded)
### ngram_range = range of the n_grams. If (1,1) only single words are considered. If (1,2) single words and 2-word groups are considered
###               If (1,3) single words, 2-word, and 3-word groups are considered. Be careful because this can create far too many positions in your vector!
### sw = if True, it will use a pre-defined stopwords list to filter out some terms.  
X_train, X_val, X_test = extract_features(proc_train, proc_val, proc_test, binary=True, max_df=10.0, min_df=0.0, ngram_range=(1,1), sw=False)

# extract labels
y_train = df_train['sentiment']
y_val = df_val['sentiment']
y_test = df_val['sentiment']

# example of a sklearn classifier (you can try with other examples from the practical session)
# in the first code block here we also import several others classifiers available in sklearn
# they all follow the same structure:
## 1-) create an instance of the classifier (e.g. kkn = NeighborsClassifier())
## 2-) train the model using the training data (e.g. knn.fit(X_train, y_train))
## 3-) generate predictions for the test set (e.g. preds = knn.predict(X_test))
## more information about sklearn classifiers: https://scikit-learn.org/stable/supervised_learning.html
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_preds_val = knn.predict(X_val)

# evaluating the model in the validation set (you do not have gold values for test set here!)
print("Predicted values for the validation set:")
print(knn_preds_val)
evaluate_classifier(y_val, knn_preds_val)
print(y_val)

# predicting the test set
#we can't assess how well these have done in this notebook for that we
#will have to submit them to the codalab competition system (see below)
knn_preds_test = knn.predict(X_test)

print("Predicted values for the test set:")
print(knn_preds_test)

#Convert the 'knn_preds_test' array of 'negative' and
#'positive's into an answer.zip file. This will also
#try to allow you to download the file. 
create_zip(knn_preds_test)

"""# Logistic Regression Classifier

Below is a classifier using a logistic reression algorithm. This is much better than the above solutions with an accuracy of 91% when ngram_Range of 1,2 is used.
"""

############################## preparing data

# apply pre_proc() function to all data splits
proc_train = pre_proc(df_train['review'])
proc_val = pre_proc(df_val['review'])
proc_test = pre_proc(df_test['review'])

#extract features
X_train, X_val, X_test = extract_features(proc_train, proc_val, proc_test, binary=False, max_df=10.0, min_df=0.0, ngram_range=(1,2), sw=False)

# extract pre-known sentiment labels (used for training and evaluation)
y_train = df_train['sentiment']
y_val = df_val['sentiment']

############################ finds best parameter c for the logistic regression (based on evaluation set)

def optimise_C(X_train, y_train, X_val, y_val, C=[0.01, 0.025, 0.05, 0.25, 0.5, 1.0]):
    #optimise the parameter C using the validation data
    best_acc = 0.
    best_c = 0.
    accuracies = []
    for c in C:
    
        lr = LogisticRegression(C=c, multi_class='auto', solver='liblinear')
        lr.fit(X_train, y_train)
        cur_acc = accuracy_score(y_val, lr.predict(X_val))
        print ("Accuracy for C=%s: %s" % (c, cur_acc))
        accuracies.append(cur_acc)
        if cur_acc > best_acc:
            best_c = c
            best_acc = cur_acc

    print ("*** Best accuracy = %f, best C = %f" % (best_acc, best_c))
    plt.plot(np.array(C).astype('str'), accuracies, 'ro')
    return best_c

#apply the above function
ngram11_C=[0.0745, 0.075, 0.07525, 0.0754, 0.0755, 0.08, 0.09, 0.10, 0.25] #best c values for ngram_range 1,1
ngram12_C=[0.07,0.08,0.09, 0.10, 0.11,0.17] #best c values for ngram_range 1,2
ngram13_C=[0.3,0.325,0.35,0.375,0.4] #best c values for ngram_range 1,3
best_c = optimise_C(X_train, y_train, X_val, y_val, C=[0.07,0.08,0.09, 0.10, 0.11,0.17])

################################# validation set evaluation

lr = LogisticRegression(C=best_c, multi_class='auto', solver='liblinear')
lr.fit(X_train, y_train) #fits model according to training data
lr_preds = lr.predict(X_val) #predict class labels 
evaluate_classifier(y_val, lr_preds)

################################# test set predictions

lrt = LogisticRegression(C=best_c,multi_class='auto', solver='liblinear')
lrt.fit(X_train,y_train)
lrt_preds=lrt.predict(X_test)

################################create zip
create_zip(lrt_preds)